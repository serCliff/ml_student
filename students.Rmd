---
title: "Clasificación Binaria"
subtitle: "Estudiantes de Portugués"
author: "Sergio Del Castillo Baranda"
date: "4/10/2020"
output: pdf_document
---


# Carga de los datos y librerías

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readtext)
library(gplots)
library(dummies)
library(dplyr)
library(bnpa)
library(MASS)
library(NeuralNetTools)
library(caret)
```


``` {r}
students.csv <- file.path(getwd(), 'student-por.csv')
STUDENTS <- read.csv2(file = students.csv, header = TRUE, sep = ';')

summary(STUDENTS)
```


# SELECCIÓN DE VARIABLES

El objetivo de este apartado es obtener las mejores variables que nos permitan optimizar nuestro modelos. El trabajo lo realizaremos en dos fases, una fase inicial en la que vamos a realizar una limpieza de datos para obtener un dataset con el que podamos generar un modelo y en segundo lugar lo que realizaremos selección de las  mejores variables para optimizar nuestro modelo.


#### LIMPIEZA DE NA
No realizamos supresión de NA dado que no hay ninguno en el fichero.

``` {r}
check.na(STUDENTS)
```

Para comenzar a trabajar con las variables vamos a hacer una selección en función del tipo de variable que es, a continuación trabajaremos con las variables de forma diferente en función de la clase de variable que sea. 

Lo primero que haremos será la selección de la variable objetivo (higher) y la separamos del dataset. A continuación, haremos una subdivisión de las columnas restantes entre continuas y categóricas almacenando los nombres de las columnas en dos variables.

``` {r}
vardep <- "higher"
students.bis <- STUDENTS[,-which(names(STUDENTS) == vardep)]

continuas <- names(select_if(students.bis, is.integer))
categoricas <- names(select_if(students.bis, is.factor))


cat("Nuestra variable objetivo será: ",vardep, "\n\nVariables continuas: ",continuas, "\n\nVariables categoricas: ",categoricas)
```

#### CREACIÓN DE VARIABLES DUMMY

Generamos variables dummy a partir de nuestras variables categóricas. En nuestro caso lo realizamos de todas dado que las variables categóricas no contienen un número demasiado elevado de valores diferentes.

``` {r echo=FALSE}
students.df<- dummy.data.frame(STUDENTS, categoricas, sep = ".")
```


#### ESTANDARIZACIÓN DE VARIABLES

A continuación estandarizamos las variables continuas. Para ello realizamos la media y desviación típica de las contínuas y a continuación las estandarizamos. Para trabajar ahora con todas las variables como continuas, las uno a las variables dummy generadas en el paso anterior.

```{r}

means <- apply(students.df[,continuas],2,mean) 
sds <- sapply(students.df[,continuas],sd) 


students.df.bis <- scale(students.df[,continuas], center = means, scale = sds)
numerocont <- which(colnames(students.df) %in% continuas)
students.df.s <- cbind(students.df.bis, students.df[,-numerocont])
```


#### SELECCIÓN DE VARIABLES

El primer paso en la selección de las variables es suprimir de las variables dummy una variable, dado que esta puede ser obtenida como una negación del resto de las variables.


``` {r}
continuas <- c("age", "Medu", "Fedu", "traveltime", "studytime", "failures", 
"famrel", "freetime", "goout", "Dalc", "Walc", "health", "absences", 
"G1", "G2", "G3", "school.GP", "sex.M", 
"address.R", "famsize.GT3", "Pstatus.A", 
"Mjob.health", "Mjob.other", "Mjob.services", 
"Mjob.teacher", "Fjob.health", "Fjob.other", 
"Fjob.services", "Fjob.teacher", "reason.course", "reason.home", 
"reason.reputation", "guardian.father", "guardian.mother", 
"schoolsup.yes", 
"famsup.yes", "paid.yes", "activities.yes", 
"nursery.yes", "higher", "internet.yes", 
"romantic.yes")

categoricas <- c("")


numerocont <- which(colnames(students.df.s) %in% continuas)
students.df.s <- students.df.s[,numerocont]


students.df.s$higher<-ifelse(students.df.s$higher=="yes","Yes","No") # Corrección de los datos para que se ajusten a los requisitos de los métodos que se aplicarán

cat("Variables continuas: ",continuas, "\n\nVariables categoricas: ",categoricas)
```

##### SELECCIÓN DE VARIABLES EN CLASIFICACIÓN BINARIA LOGÍSTICA

Para la selección de variables hacemos la búsqueda mediante el uso de la medida de ajuste AIC. Para ejecutar los algoritmos lo realizaremos mediante el método stepwise que que va incluyendo y sacando variables con el objetivo de optimizar la selección.


``` {r}
full<-glm(factor(higher)~., data=students.df.s, family = binomial(link="logit"))
null<-glm(factor(higher)~1, data=students.df.s, family = binomial(link="logit"))

seleccion<-stepAIC(null,scope=list(upper=full),direction="both", trace=0)

variables <- names(seleccion$coefficients)[-1]
cat("La mejor selección de variables viene dada por: ", variables)
```


## GENERACIÓN DE LOS SETS DE DATOS (train, test / Validación cruzada)

En el anterior apartado hemos obtenido las mejores variables para poder generar nuestros modelos. En este apartado lo que vamos a realizar es una división de los datos en dos sets, uno para la parte de test y otro para la parte de entrenamiento del modelo. El objetivo es utilizar el set de entrenamiento para entrenar nuestro modelo y prepararlo para la predicción y realizar pruebas para comprobar la eficacia con la que es capaz de predecir sobre nuestro set de test.

La validación de los datos la realizaremos mediante validación cruzada que lo que realiza es la selección del mejor conjunto de datos que formarán parte de cada set mediante la comprobación redundante de diferentes escenarios de manera que los datos que queden en un set y otro estén lo más balanceados posible.

Utilizaremos validación cruzada repetida dado que únicamente tenemos un set de 500 filas de datos. La generación de los sets de train y test se realiza 4 veces
``` {r}
set.seed(1234)
control<-trainControl(method = "repeatedcv",number=4,savePredictions = "all")
```

## COMPARACIÓN DE MODELOS
#### MODELO CON REGRESIÓN LINEAL

Modelo con regresión lineal, este no tendrá rejilla porque no tiene hiperparámetros.

``` {r}
reg<- train(factor(higher)~G1+age+studytime+G3+school.GP+famsup.yes+Mjob.health+schoolsup.yes+Walc+famrel+Fedu,
                data=students.df.s,
                method="glm",
                trControl=control,
                trace=FALSE)
reg
```

#### MODELO CON RED NEURONAL

Ahora vamos a generar un modelo con redes neuronales. Para comprobar su eficacia realizaremos diferentes tuneos hasta obtener el mejor resultado. La forma que tenemos de realizar el tuneado mediante el uso de una rejilla.

``` {r}
nnetgrid <-  expand.grid(size=c(1,2,3,5,10),
                         decay=c(0.01,0.1,0.001),
                         bag=FALSE)

rednnet<- train(factor(higher)~G1+age+studytime+G3+school.GP+famsup.yes+Mjob.health+schoolsup.yes+Walc+famrel+Fedu,
                data=students.df.s,
                method="avNNet",linout = FALSE,
                maxit=100,
                trControl=control,
                tuneGrid=nnetgrid,
                repeats=5,
                verbose=FALSE,
                trace=FALSE)
rednnet


bestTuneNnet <- function(nnetmodel, size=FALSE, decay=FALSE){
  # Función que ayuda a obtener el mejor resultado obtenido en un modelo NEURAL NET
  bestSize <- rednnet$bestTune$size
  bestDecay <- rednnet$bestTune$decay
  # Cojo los parámetros de la función si están establecidos
  if (size != FALSE) {bestSize <- size}
  if (decay != FALSE) {bestDecay <- decay}
  nnetmodel$results[nnetmodel$results$size == bestSize & 
                      nnetmodel$results$decay == bestDecay,]
}
```

#### BAGGING

``` {r}
set.seed(1234)
baggrid<-expand.grid(mtry=c(11)) # El número de variables independientes

bag<- train(factor(higher)~G1+age+studytime+G3+school.GP+famsup.yes+Mjob.health+schoolsup.yes+Walc+famrel+Fedu,
           data=students.df.s,
           method="rf",
           trControl=control,
           tuneGrid=baggrid,
           linout=FALSE,
           nodesize=10,
           ntree=5000,
           sampsize=200,
           replace=TRUE,
           trace=FALSE)
bag

```

#### RANDOM FOREST

``` {r}
set.seed(1234)
rfgrid<-expand.grid(mtry=seq(3, 11, by = 2))

rf<- train(factor(higher)~G1+age+studytime+G3+school.GP+famsup.yes+Mjob.health+schoolsup.yes+Walc+famrel+Fedu,
           data=students.df.s,
           method="rf",
           trControl=control,
           tuneGrid=rfgrid,
           linout = FALSE,ntree=5000,nodesize=10,
           replace=TRUE,
           importance=TRUE,
           trace=FALSE)

rf

bestTuneRf <-  function(rfmodel, mtry=FALSE){
  # Función que ayuda a obtener el mejor resultado obtenido en un modelo RANDOM FOREST
  bMtry <- rfmodel$bestTune$mtry
  # Cojo los parámetros de la función si están establecidos
  if (mtry != FALSE) {bMtry <- mtry}
  rfmodel$results[rfmodel$results$mtry == bMtry,]  
}

```

#### GRADIENT BOOSTING

``` {r}
set.seed(1234)
gbmgrid<-expand.grid(shrinkage=c(0.1,0.05,0.01),
                     n.minobsinnode=c(10,20),
                     n.trees=c(100,500,1000),
                     interaction.depth=c(1,2,3))

gbm<- train(factor(higher)~G1+age+studytime+G3+school.GP+famsup.yes+Mjob.health+schoolsup.yes+Walc+famrel+Fedu,
            data=students.df.s,
            method="gbm",
            trControl=control,
            tuneGrid=gbmgrid,
            distribution="bernoulli", 
            bag.fraction=1,
            verbose=FALSE)

gbm


bestTuneGbm <-  function(gbmModel, n.trees=FALSE, shrinkage=FALSE, n.minobsinnode=FALSE, interaction.depth=FALSE){
  # Función que ayuda a obtener el mejor resultado obtenido en un modelo GRADIENT BOOSTING MACHINE
  bTrees <- gbmModel$bestTune$n.trees
  bShrink <- gbmModel$bestTune$shrinkage
  bMin <- gbmModel$bestTune$n.minobsinnode
  bInt <- gbmModel$bestTune$interaction.depth
  # Cojo los parámetros de la función si están establecidos
  if (n.trees != FALSE) {bTrees <- n.trees}
  if (shrinkage != FALSE) {bShrink <- shrinkage}
  if (n.minobsinnode != FALSE) {bMin <- n.minobsinnode}
  if (interaction.depth != FALSE) {bInt <- interaction.depth}
  #Devuelve el mejor resultado para los parámetros introducidos
  gbmModel$results[gbmModel$results$n.trees == bTrees & 
                     gbmModel$results$shrinkage == bShrink & 
                     gbmModel$results$n.minobsinnode == bMin & 
                     gbmModel$results$interaction.depth == bInt,]  
}

```



#### XGBOOST

``` {r}
set.seed(1234)
xgbmgrid<-expand.grid(
  min_child_weight=c(10),
  eta=c(0.1,0.05,0.03,0.01),
  nrounds=c(100,500,1000),
  max_depth=6,gamma=0,colsample_bytree=1,subsample=1)

xgbm<- train(factor(higher)~G1+age+studytime+G3+school.GP+famsup.yes+Mjob.health+schoolsup.yes+Walc+famrel+Fedu,
             data=students.df.s,
             method="xgbTree",
             trControl=control,
             tuneGrid=xgbmgrid,
             verbose=FALSE)

xgbm


bestTuneXgbm <-  function(XgbmModel, 
                          nrounds = FALSE, 
                          max_depth = FALSE, 
                          eta = FALSE, 
                          gamma = FALSE, 
                          colsample_bytree = FALSE, 
                          min_child_weight = FALSE, 
                          subsample = FALSE){
  
  # Función que ayuda a obtener el mejor resultado obtenido en un modelo XGBOOST
  
  bnrounds <- XgbmModel$bestTune$nrounds
  bmax_depth <- XgbmModel$bestTune$max_depth
  beta <- XgbmModel$bestTune$eta
  bgamma <- XgbmModel$bestTune$gamma
  bcolsample_bytree <- XgbmModel$bestTune$colsample_bytree
  bmin_child_weight <- XgbmModel$bestTune$min_child_weight
  bsubsample <- XgbmModel$bestTune$subsample
  
  # Cojo los parámetros de la función si están establecidos
  if (nrounds != FALSE) { bnrounds <- nrounds }
  if (max_depth != FALSE) { bmax_depth <- max_depth }
  if (eta != FALSE) { beta <- eta }
  if (gamma != FALSE) { bgamma <- gamma }
  if (colsample_bytree != FALSE) { bcolsample_bytree <- colsample_bytree }
  if (min_child_weight != FALSE) { bmin_child_weight <- min_child_weight }
  if (subsample != FALSE) { bsubsample <- subsample }
  
  #Devuelve el mejor resultado para los parámetros introducidos
  XgbmModel$results[XgbmModel$results$nrounds == bnrounds &
                      XgbmModel$results$max_depth == bmax_depth &
                      XgbmModel$results$eta == beta &
                      XgbmModel$results$gamma == bgamma &
                      XgbmModel$results$colsample_bytree == bcolsample_bytree &
                      XgbmModel$results$min_child_weight == bmin_child_weight &
                      XgbmModel$results$subsample == bsubsample,]
}

```

#### SUPPORT VECTOR MACHINE - LINEAR

``` {r}
set.seed(1234)

SVMgrid<-expand.grid(C=c(0.01,0.1,0.2,0.3,0.5,1,2))

SVMl<- train(factor(higher)~G1+age+studytime+G3+school.GP+famsup.yes+Mjob.health+schoolsup.yes+Walc+famrel+Fedu,
            data=students.df.s,
            method="svmLinear",
            trControl=control,
            tuneGrid=SVMgrid,
            verbose=FALSE)

SVMl


bestTuneSVMl <-  function(svmlmodel, C=FALSE){
  # Función que ayuda a obtener el mejor resultado obtenido en un modelo SUPPORT VECTOR MACHINE - LINEAR
  bC <- svmlmodel$bestTune$C
  # Cojo los parámetros de la función si están establecidos
  if (C != FALSE) {bC <- C}
  svmlmodel$results[svmlmodel$results$C == bC,]  
}

```

#### SUPPORT VECTOR MACHINE - POLYNOMIAL

``` {r}
set.seed(1234)
SVMgrid<-expand.grid(degree=c(1,2,3),
                     scale=c(5:7),
                     C=c(3:6))


SVMp<- train(factor(higher)~G1+age+studytime+G3+school.GP+famsup.yes+Mjob.health+schoolsup.yes+Walc+famrel+Fedu,
            data=students.df.s,
            method="svmPoly",
            trControl=control,
            tuneGrid=SVMgrid,
            verbose=FALSE)
SVMp


bestTuneSVMp <-  function(svmpmodel, degree=FALSE, scale=FALSE, C=FALSE){
  # Función que ayuda a obtener el mejor resultado obtenido en un modelo SUPPORT VECTOR MACHINE - POLYNOMIAL
  bDegree <- svmpmodel$bestTune$degree
  bScale <- svmpmodel$bestTune$scale
  bC <- svmpmodel$bestTune$C
  # Cojo los parámetros de la función si están establecidos
  if (degree != FALSE) {bDegree <- degree}
  if (scale != FALSE) {bScale <- scale}
  if (C != FALSE) {bC <- C}
  svmpmodel$results[svmpmodel$results$degree == bDegree &
                      svmpmodel$results$scale == bScale &
                      svmpmodel$results$C == bC,]  
}

```


#### SUPPORT VECTOR MACHINE - RADIAL

``` {r}
set.seed(1234)
SVMgrid<-expand.grid(sigma=c(0.01,0.05,0.1,1),
                     C=c(1:5))

SVMr<- train(factor(higher)~G1+age+studytime+G3+school.GP+famsup.yes+Mjob.health+schoolsup.yes+Walc+famrel+Fedu,
            data=students.df.s,
            method="svmRadial",
            trControl=control,
            tuneGrid=SVMgrid,
            verbose=FALSE)
SVMr


bestTuneSVMr <-  function(svmrmodel, sigma=FALSE, C=FALSE){
  # Función que ayuda a obtener el mejor resultado obtenido en un modelo SUPPORT VECTOR MACHINE - RADIAL
  bSigma <- svmrmodel$bestTune$sigma
  bC <- svmrmodel$bestTune$C
  # Cojo los parámetros de la función si están establecidos
  if (sigma != FALSE) {bSigma <- sigma}
  if (C != FALSE) {bC <- C}
  svmrmodel$results[svmrmodel$results$sigma == bSigma &
                      svmrmodel$results$C == bC,]  
}

```



Realizamos una comparativa de la precisión todos los modelos anteriores

``` {r}
nnettune <- bestTuneNnet(rednnet)
bagtune <- bag$results
rftune <- bestTuneRf(rf)
gbmtune <- bestTuneGbm(gbm)
xgbmtune <- bestTuneXgbm(xgbm)
svmltune <- bestTuneSVMl(SVMl)
svmptune <- bestTuneSVMp(SVMp)
svmrtune <- bestTuneSVMr(SVMr)

models = c(reg$method, 
           rednnet$method, 
           'bagging', 
           rf$method, 
           gbm$method, 
           xgbm$method, 
           SVMl$method,
           SVMp$method,
           SVMr$method)

accuracies = c(reg$results$Accuracy, 
               nnettune$Accuracy, 
               bagtune$Accuracy, 
               rftune$Accuracy, 
               gbmtune$Accuracy, 
               xgbmtune$Accuracy,
               svmltune$Accuracy,
               svmptune$Accuracy,
               svmrtune$Accuracy)
  
comparation <- data.frame("Model" = models, "Accuracy" = accuracies)
comparation[order(comparation$Accuracy, decreasing = TRUE),]
```



# VOY POR AQUÍ, FALTA SEGUIR AJUSTANDO LOS MODELOS PARA METER LOS TUNEOS OBTENIDOS EN EL SIGUIENTE APARTADO

## PREPARACIÓN DE MODELOS PARA ENSAMBLADO
``` {r}
source ("library/cruzadas avnnet y log binaria.R")
source ("library/cruzada arbolbin.R")
source ("library/cruzada rf binaria.R")
source ("library/cruzada gbm binaria.R")
source ("library/cruzada xgboost binaria.R")
source ("library/cruzada SVM binaria lineal.R")
source ("library/cruzada SVM binaria polinomial.R")
source ("library/cruzada SVM binaria RBF.R")


logi<-cruzadalogistica(data=students.df.s,
 vardep=vardep,listconti=variables,
 listclass=c(""), grupos=4,sinicio=1234,repe=5)

logi$modelo="Logística"


# Tuneamos con los datos obtenidos en el ajuste anterior
avnet<-cruzadaavnnetbin(data=students.df.s,
 vardep=vardep,listconti=variables,
 listclass=c(""), grupos=4,sinicio=1234,repe=5,
 size=c(nnettune$size),decay=c(nnettune$decay))

avnet$modelo="AvNet"


bag<-cruzadarfbin(data=students.df.s,
  vardep=vardep,listconti=variables,
  listclass=c(""),
  grupos=4,sinicio=1234,
  repe=5,
  nodesize=10,
  ntree=5000,
  sampsize=200,
  replace=TRUE,
  mtry=bagtune$mtry)

bag$modelo="Bag"


rf<-cruzadarfbin(data=students.df.s, vardep=vardep,
 listconti=variables, listclass=c(""),
 grupos=4,sinicio=1234,repe=5,nodesize=10,
 mtry=rftune$mtry, # mtry obtenido en tuneo
 ntree=3000,
 replace=TRUE,
 sampsize=150)

rf$modelo="RF"


gbm<-cruzadagbmbin(data=students.df.s,
  vardep=vardep,listconti=variables,
  listclass=c(""),
  grupos=4,sinicio=1234,repe=5,
  n.minobsinnode=gbmtune$n.minobsinnode,
  shrinkage=gbmtune$shrinkage,
  n.trees=gbmtune$n.trees,
  interaction.depth=gbmtune$interaction.depth)

gbm$modelo="gbm"


xgbm<-cruzadaxgbmbin(data=students.df.s,
 vardep=vardep,listconti=variables,
  listclass=c(""),
  grupos=4,sinicio=1234,repe=5,
  min_child_weight=xgbmtune$min_child_weight,
  eta=xgbmtune$eta,
  nrounds=xgbmtune$nrounds,
  max_depth=xgbmtune$max_depth,
  gamma=xgbmtune$gamma,
  colsample_bytree=xgbmtune$colsample_bytree,
  subsample=xgbmtune$subsample)

xgbm$modelo="xgbm"

svm<-cruzadaSVMbin(data=students.df.s,
 vardep=vardep,listconti=variables,
listclass=c(""),
  grupos=4,sinicio=1234,repe=5,
  C=svmltune$C) # Parámetro obtenido del tuneo

svm$modelo="SVM"


svmp<-cruzadaSVMbinPoly(data=students.df.s,
 vardep=vardep,listconti=variables,
listclass=c(""),
  grupos=4,sinicio=1234,repe=5,
  C=svmptune$C,
  degree=svmptune$degree,
  scale=svmptune$scale)

svmp$modelo="SVMPoly"


svmrbf<-cruzadaSVMbinRBF(data=students.df.s, vardep=vardep,
   listconti=variables,
 listclass=c(""),
  grupos=4,sinicio=1234,repe=5,
  C=svmrtune$C,
  sigma=svmrtune$sigma)

svmrbf$modelo="SVMRBF"



union<-rbind(logi,avnet,bag,rf, gbm, xgbm, svm, svmp, svmrbf)

par(cex.axis=0.6, cex=1, las=1)
boxplot(data=union,tasa~modelo,main="TASA FALLOS")
boxplot(data=union,auc~modelo,main="AUC")

```

